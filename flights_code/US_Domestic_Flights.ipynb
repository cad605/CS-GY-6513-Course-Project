{
  "metadata": {
    "name": "US_Domestic_Flights",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom __future__ import print_function\n\nimport sys\nfrom operator import add\nfrom pyspark import SparkContext\nfrom pyspark.sql.functions import format_string, date_format, col\nfrom pyspark.sql.types import FloatType\nfrom csv import reader\n\nspark \u003d SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL - Flights Analysis\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfs \u003d (sc._jvm.org\n      .apache.hadoop\n      .fs.FileSystem\n      .get(sc._jsc.hadoopConfiguration())\n      )\n      \nout_path \u003d \"us_flights\"\n#data can be found at the projects github repo; however, the flights data is very large\nairport_data_path \u003d \"/user/cad605/data/airports.csv\"\nflight_data_path \u003d \"/user/cad605/data/combined_flights.csv\"\n\nfs.delete(sc._jvm.org.apache.hadoop.fs.Path(out_path), True)\n\n# TODO: update path with sys.argv[1] if running outside of Zeppelin\nairports \u003d spark.read.format(\u0027csv\u0027).options(header\u003d\u0027true\u0027, inferschema\u003d\u0027true\u0027).load(\n    airport_data_path)\n# TODO: update path with sys.argv[2] if running outside of Zeppelin\nflights \u003d spark.read.format(\u0027csv\u0027).options(header\u003d\u0027true\u0027, inferschema\u003d\u0027true\u0027).load(\n    flight_data_path)\n    \nairports.createOrReplaceTempView(\"airports\")\nflights.createOrReplaceTempView(\"flights\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#peek flight data\nflights \u003d spark.sql(\"SELECT * FROM flights\")\nflights.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#peek airports data\nairports \u003d spark.sql(\"SELECT * FROM airports\")\nairports.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#filter our airports to only those within the United States, and that are large, commmercial airports (ex. no Air Force bases)\nfiltered_airports \u003d spark.sql(\"SELECT ident, type, name, latitude_deg, longitude_deg, iso_country, iso_region, municipality FROM airports WHERE (iso_country\u003d\u0027US\u0027 AND type\u003d\u0027large_airport\u0027 AND (name NOT LIKE \u0027%Air Force%\u0027))\")\nfiltered_airports.show()\nfiltered_airports.createOrReplaceTempView(\"filtered_airports\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#filter flights to only domestic flights, where the origin and destination match an airport within our filtered_airport table\nfiltered_flights \u003d spark.sql(\"SELECT f.callsign, f.origin, f.destination, f.day, a1.name as origin_name, a1.latitude_deg as origin_lat, a1.longitude_deg as origin_long, a1.iso_region as origin_region, a1.municipality as origin_municipality, a2.name as dest_name, a2.latitude_deg as dest_lat, a2.longitude_deg as dest_long, a2.iso_region as dest_region, a2.municipality as dest_municipality FROM flights as f INNER JOIN filtered_airports AS a1 ON f.origin \u003d a1.ident INNER JOIN filtered_airports as a2 ON f.destination \u003d a2.ident\")\nfiltered_flights.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#write combined dataset, US Domestic Flights, to output file: us_flights\nfiltered_flights.select(format_string(\u0027%s, %s, %s, %s, %s, %f, %f, %s, %s, %s, %f, %f, %s, %s\u0027, filtered_flights.callsign, filtered_flights.origin, filtered_flights.destination,date_format(filtered_flights.day, \u0027yyyy-MM-dd\u0027), filtered_flights.origin_name, filtered_flights.origin_lat, filtered_flights.origin_long, filtered_flights.origin_region, filtered_flights.origin_municipality, filtered_flights.dest_name, filtered_flights.dest_lat, filtered_flights. dest_long, filtered_flights.dest_region, filtered_flights.dest_municipality)).write.save(out_path, format\u003d\"text\")"
    }
  ]
}